{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7f6c08-4f83-408b-902e-b0da5f36367d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/testsynapsehieum has been unmounted.\n",
      "Mounted /mnt/testsynapsehieum successfully\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": os.getenv(\"AZURE_CLIENT_ID\"),  \n",
    "    \"fs.azure.account.oauth2.client.secret\": os.getenv(\"AZURE_CLIENT_SECRET\"), \n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{os.getenv('AZURE_TERNANT_ID')}/oauth2/v2.0/token\" \n",
    "}\n",
    "container = os.getenv(\"AZURE_CONTAINER_NAME\")\n",
    "storage_account = os.getenv(\"AZURE_STORAGE_ACCOUNT\")\n",
    "mount_point = f\"/mnt/{os.getenv('AZURE_CONTAINER_NAME')}\"   \n",
    "\n",
    "existing_mounts = [mnt.mountPoint for mnt in dbutils.fs.mounts()]\n",
    "if mount_point in existing_mounts:\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=f\"abfss://{container}@{storage_account}.dfs.core.windows.net/\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n",
    "print(f\"Mounted {mount_point} successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e55e2b-87f7-4f10-8c1a-385920180ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Employee - Schema:\n",
      "root\n",
      " |-- EmployeeID: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- OverTime: string (nullable = true)\n",
      " |-- HireDate: date (nullable = true)\n",
      "\n",
      "Processing Job - Schema:\n",
      "root\n",
      " |-- JobID: string (nullable = true)\n",
      " |-- JobRole: string (nullable = true)\n",
      "\n",
      "Processing Department - Schema:\n",
      "root\n",
      " |-- DepartmentID: string (nullable = true)\n",
      " |-- DepartmentName: string (nullable = true)\n",
      "\n",
      "Processing Shift - Schema:\n",
      "root\n",
      " |-- ShiftID: string (nullable = true)\n",
      " |-- ShiftName: string (nullable = true)\n",
      " |-- Start: integer (nullable = true)\n",
      " |-- End: integer (nullable = true)\n",
      "\n",
      "Processing BusinessTravel - Schema:\n",
      "root\n",
      " |-- BusinessTravelID: string (nullable = true)\n",
      " |-- BusinessTravelOption: string (nullable = true)\n",
      "\n",
      "Processing Location - Schema:\n",
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- PostalCode: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "Processing Training - Schema:\n",
      "root\n",
      " |-- TrainingID: string (nullable = true)\n",
      " |-- TrainingName: string (nullable = true)\n",
      " |-- TrainingCostPerHour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, row_number, max, to_date\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ETL_Fact_Dim_Tables\").getOrCreate()\n",
    "\n",
    "# Define storage paths\n",
    "mount_point = \"/mnt/testsynapsehieum\"\n",
    "silver_path = f\"{mount_point}/silver\"\n",
    "gold_path = f\"{mount_point}/gold\"\n",
    "\n",
    "# Load data from Silver Layer\n",
    "silver_tables = {\n",
    "    \"Employee\": spark.read.format(\"parquet\").load(f\"{silver_path}/HR.Employee\"),\n",
    "    \"Job\": spark.read.format(\"parquet\").load(f\"{silver_path}/HR.Job\"),\n",
    "    \"Department\": spark.read.format(\"parquet\").load(f\"{silver_path}/HR.Department\"),\n",
    "    \"Shift\": spark.read.format(\"parquet\").load(f\"{silver_path}/HR.Shift\"),\n",
    "    \"BusinessTravel\": spark.read.format(\"parquet\").load(f\"{silver_path}/HR.BusinessTravel\"),\n",
    "    \"Location\": spark.read.format(\"parquet\").load(f\"{silver_path}/HR.Location\"),\n",
    "    \"Training\": spark.read.format(\"parquet\").load(f\"{silver_path}/HR.Training\"),\n",
    "}\n",
    "\n",
    "# Define Dimension Tables and Surrogate Keys\n",
    "dim_columns = {\n",
    "    \"BusinessTravel\": [\"BusinessTravelID\", \"BusinessTravelOption\"],\n",
    "    \"Employee\": [\"EmployeeID\", \"FirstName\", \"LastName\", \"FullName\", \"Age\", \"Gender\", \"MaritalStatus\", \"OverTime\", \"HireDate\"],\n",
    "    \"Location\": [\"LocationID\", \"State\", \"PostalCode\", \"Country\"],\n",
    "    \"Job\": [\"JobID\", \"JobRole\"],\n",
    "    \"Training\": [\"TrainingID\", \"TrainingName\", \"TrainingCostPerHour\"],\n",
    "    \"Department\": [\"DepartmentID\", \"DepartmentName\"],\n",
    "    \"Shift\": [\"ShiftID\", \"ShiftName\", \"Start\", \"End\"]\n",
    "}\n",
    "\n",
    "# Function to update SCD Type 2 for a table\n",
    "def update_scd_type_2(table_name, new_df, key_column):\n",
    "    table_path = f\"{gold_path}/Dim_{table_name}\"\n",
    "\n",
    "    # Check if table exists\n",
    "    if DeltaTable.isDeltaTable(spark, table_path):\n",
    "        existing_df = spark.read.format(\"delta\").load(table_path)\n",
    "        \n",
    "        # Ensure key column exists\n",
    "        if f\"{table_name}Key\" in existing_df.columns:\n",
    "            max_surrogate = existing_df.agg(max(col(f\"{table_name}Key\"))).collect()[0][0] or 0\n",
    "    else:\n",
    "        existing_df = spark.createDataFrame([], new_df.schema)  # Empty DataFrame\n",
    "\n",
    "    # Ensure schema consistency\n",
    "    new_df = new_df.withColumn(key_column, col(key_column).cast(\"string\"))\n",
    "    existing_df = existing_df.withColumn(key_column, col(key_column).cast(\"string\"))\n",
    "\n",
    "    # Debugging Schema\n",
    "    print(f\"Processing {table_name} - Schema:\")\n",
    "    new_df.printSchema()\n",
    "\n",
    "    # Remove duplicates\n",
    "    new_df = new_df.dropDuplicates([key_column])\n",
    "\n",
    "    # Left anti join to find new records\n",
    "    changed_records = new_df.join(existing_df, on=[key_column], how=\"left_anti\") \\\n",
    "                        .withColumn(\"StartDate\", current_timestamp()) \\\n",
    "                        .withColumn(\"EndDate\", to_date(lit(\"9999-12-31\"))) \\\n",
    "                        .withColumn(f\"{table_name}Key\", col(key_column).cast(\"int\"))  # Keep EmployeeID as Key\n",
    "\n",
    "    if DeltaTable.isDeltaTable(spark, table_path):\n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        delta_table.alias(\"old\").merge(\n",
    "            changed_records.alias(\"new\"),\n",
    "            f\"old.{key_column} = new.{key_column}\"\n",
    "        ).whenMatchedUpdate(set={\n",
    "            \"EndDate\": current_timestamp()\n",
    "        }).whenNotMatchedInsertAll().execute()\n",
    "    else:\n",
    "        changed_records.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "# Apply SCD Type 2 for All Dimension Tables\n",
    "for table, df in silver_tables.items():\n",
    "    if table in dim_columns:\n",
    "        update_scd_type_2(table, df.select(*dim_columns[table]), dim_columns[table][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca04bbf-0420-48b4-a270-19856bf77987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, date_format, year, month, dayofmonth, quarter, lit, dayofweek, hour, minute, second, current_timestamp\n",
    "from datetime import datetime, timedelta\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DimTime\").getOrCreate()\n",
    "\n",
    "# Define storage paths\n",
    "gold_path = \"/mnt/testsynapsehieum/gold\"\n",
    "dim_time_path = f\"{gold_path}/Dim_Time\"\n",
    "\n",
    "# Create Date Range\n",
    "start_date = datetime(1988, 1, 1)\n",
    "end_date = datetime(2030, 12, 31)\n",
    "\n",
    "date_list = [(start_date + timedelta(days=x)).strftime('%Y-%m-%d') for x in range((end_date - start_date).days + 1)]\n",
    "df = spark.createDataFrame([(d,) for d in date_list], [\"Date\"])\n",
    "\n",
    "# Convert Date column to date type\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "# Generate Time Dimension Columns\n",
    "df_dim_time = df.withColumn(\"TimeKey\", date_format(\"Date\", \"yyyyMMdd\").cast(\"int\")) \\\n",
    "                .withColumn(\"Year\", year(\"Date\")) \\\n",
    "                .withColumn(\"Quarter\", quarter(\"Date\")) \\\n",
    "                .withColumn(\"QuarterName\", expr(\"concat('Q', quarter(Date))\")) \\\n",
    "                .withColumn(\"Month\", month(\"Date\")) \\\n",
    "                .withColumn(\"MonthName\", date_format(\"Date\", \"MMMM\")) \\\n",
    "                .withColumn(\"MonthNameShort\", date_format(\"Date\", \"MMM\")) \\\n",
    "                .withColumn(\"Day\", dayofmonth(\"Date\")) \\\n",
    "                .withColumn(\"DayOfWeek\", dayofweek(\"Date\")) \\\n",
    "                .withColumn(\"StartYear\", expr(\"make_date(Year, 1, 1)\")) \\\n",
    "                .withColumn(\"EndYear\", expr(\"make_date(Year, 12, 31)\")) \\\n",
    "                .withColumn(\"StartQuarter\", expr(\"date_trunc('QUARTER', Date)\")) \\\n",
    "                .withColumn(\"EndQuarter\", expr(\"last_day(date_add(StartQuarter, 89))\")) \\\n",
    "                .withColumn(\"StartMonth\", expr(\"date_trunc('MONTH', Date)\")) \\\n",
    "                .withColumn(\"EndMonth\", expr(\"last_day(Date)\")) \\\n",
    "                .withColumn(\"IsHoliday\", lit(0))  # Default holiday flag\n",
    "\n",
    "# Check if Delta Table Exists\n",
    "if DeltaTable.isDeltaTable(spark, dim_time_path):\n",
    "    delta_table = DeltaTable.forPath(spark, dim_time_path)\n",
    "\n",
    "    # Perform Incremental Merge\n",
    "    delta_table.alias(\"old\").merge(\n",
    "        df_dim_time.alias(\"new\"),\n",
    "        \"old.TimeKey = new.TimeKey\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "else:\n",
    "    # First-time full load\n",
    "    df_dim_time.write.format(\"delta\").mode(\"overwrite\").save(dim_time_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07b7703-947d-43bb-9411-ac70afb6d6cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact_Performance_Employee processed successfully.\n",
      "Fact_HR_Training processed successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, row_number, current_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create Fact_Performance_Employee Table\n",
    "def create_fact_performance_employee():\n",
    "    dim_employee = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Employee\").alias(\"dim_employee\")\n",
    "    dim_job = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Job\").alias(\"dim_job\")\n",
    "    dim_department = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Department\").alias(\"dim_department\")\n",
    "    dim_shift = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Shift\").alias(\"dim_shift\")\n",
    "    dim_business_travel = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_BusinessTravel\").alias(\"dim_business_travel\")\n",
    "    dim_location = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Location\").alias(\"dim_location\")\n",
    "\n",
    "    df = silver_tables[\"Employee\"].alias(\"emp\") \\\n",
    "        .join(dim_employee, col(\"emp.EmployeeID\") == col(\"dim_employee.EmployeeID\"), \"left\") \\\n",
    "        .join(dim_job, col(\"emp.JobID\") == col(\"dim_job.JobID\"), \"left\") \\\n",
    "        .join(dim_department, col(\"emp.DepartmentID\") == col(\"dim_department.DepartmentID\"), \"left\") \\\n",
    "        .join(dim_shift, col(\"emp.ShiftID\") == col(\"dim_shift.ShiftID\"), \"left\") \\\n",
    "        .join(dim_business_travel, col(\"emp.BusinessTravelID\") == col(\"dim_business_travel.BusinessTravelID\"), \"left\") \\\n",
    "        .join(dim_location, col(\"emp.LocationID\") == col(\"dim_location.LocationID\"), \"left\") \\\n",
    "        .select(\n",
    "            col(\"dim_employee.EmployeeKey\").alias(\"Employee_Key\"),\n",
    "            col(\"dim_job.JobKey\").alias(\"Job_Key\"),\n",
    "            col(\"dim_department.DepartmentKey\").alias(\"Department_Key\"),\n",
    "            col(\"dim_shift.ShiftKey\").alias(\"Shift_Key\"),\n",
    "            col(\"dim_business_travel.BusinessTravelKey\").alias(\"BusinessTravel_Key\"),\n",
    "            col(\"dim_location.LocationKey\").alias(\"Location_Key\"),\n",
    "            col(\"emp.HireDate\").alias(\"Time_Key\"),\n",
    "            col(\"emp.MonthlyIncome\").alias(\"Monthly_Salary\"),\n",
    "            col(\"emp.PerformanceRating\").alias(\"Performance_Score\"),\n",
    "            col(\"emp.JobInvolvement\").alias(\"Projects_Handled\"),\n",
    "            col(\"emp.OverTime\").alias(\"Overtime_Hours\"),\n",
    "            col(\"emp.SickDays\").alias(\"Sick_Days\"),\n",
    "            col(\"emp.JobSatisfaction\"),\n",
    "            col(\"emp.YearsAtCompany\"),\n",
    "            col(\"emp.CurrentWorking\"),\n",
    "            col(\"emp.JobLevel\"),\n",
    "            col(\"emp.modified_date\"),\n",
    "            col(\"emp.is_deleted\")\n",
    "        ).withColumn(\"FactPerformanceEmployeeKey\", row_number().over(Window.orderBy(lit(1))))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Incremental Load for Fact_Performance_Employee\n",
    "fact_performance_employee_table_path = f\"{gold_path}/Fact_Performance_Employee\"\n",
    "fact_performance_employee = create_fact_performance_employee()\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, fact_performance_employee_table_path):\n",
    "    delta_table = DeltaTable.forPath(spark, fact_performance_employee_table_path)\n",
    "    delta_table.alias(\"old\").merge(\n",
    "        fact_performance_employee.alias(\"new\"),\n",
    "        \"old.Employee_Key = new.Employee_Key AND old.Job_Key = new.Job_Key AND old.Department_Key = new.Department_Key\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"Monthly_Salary\": col(\"new.Monthly_Salary\"),\n",
    "        \"Performance_Score\": col(\"new.Performance_Score\"),\n",
    "        \"Projects_Handled\": col(\"new.Projects_Handled\"),\n",
    "        \"Overtime_Hours\": col(\"new.Overtime_Hours\"),\n",
    "        \"Sick_Days\": col(\"new.Sick_Days\"),\n",
    "        \"JobSatisfaction\": col(\"new.JobSatisfaction\"),\n",
    "        \"YearsAtCompany\": col(\"new.YearsAtCompany\"),\n",
    "        \"CurrentWorking\": col(\"new.CurrentWorking\"),\n",
    "        \"JobLevel\": col(\"new.JobLevel\"),\n",
    "        \"modified_date\": col(\"new.modified_date\"),\n",
    "        \"is_deleted\": col(\"new.is_deleted\")\n",
    "    }).whenNotMatchedInsertAll().execute()\n",
    "else:\n",
    "    fact_performance_employee.write.format(\"delta\").mode(\"overwrite\").save(fact_performance_employee_table_path)\n",
    "\n",
    "print(\"Fact_Performance_Employee processed successfully.\")\n",
    "\n",
    "# Create Fact_HR_Training Table\n",
    "def create_fact_hr_training():\n",
    "    dim_employee = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Employee\").alias(\"dim_employee\")\n",
    "    dim_training = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Training\").alias(\"dim_training\")\n",
    "    dim_department = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Department\").alias(\"dim_department\")\n",
    "    dim_location = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Location\").alias(\"dim_location\")\n",
    "    dim_job = spark.read.format(\"delta\").load(f\"{gold_path}/Dim_Job\").alias(\"dim_job\") \n",
    "\n",
    "    df = silver_tables[\"Employee\"].alias(\"emp\") \\\n",
    "        .join(dim_employee, col(\"emp.EmployeeID\") == col(\"dim_employee.EmployeeID\"), \"left\") \\\n",
    "        .join(dim_training, col(\"emp.TrainingID\") == col(\"dim_training.TrainingID\"), \"left\") \\\n",
    "        .join(dim_department, col(\"emp.DepartmentID\") == col(\"dim_department.DepartmentID\"), \"left\") \\\n",
    "        .join(dim_location, col(\"emp.LocationID\") == col(\"dim_location.LocationID\"), \"left\") \\\n",
    "        .join(dim_job, col(\"emp.JobID\") == col(\"dim_job.JobID\"), \"left\") \\\n",
    "        .select(\n",
    "            col(\"dim_employee.EmployeeKey\").alias(\"Employee_Key\"),\n",
    "            col(\"dim_training.TrainingKey\").alias(\"Training_Key\"),\n",
    "            col(\"dim_department.DepartmentKey\").alias(\"Department_Key\"),\n",
    "            col(\"dim_location.LocationKey\").alias(\"Location_Key\"),\n",
    "            col(\"dim_job.JobKey\").alias(\"Job_Key\"), \n",
    "            col(\"emp.TraingingTime\").alias(\"Time_Key\"),\n",
    "            col(\"dim_training.TrainingCostPerHour\").alias(\"Training_Cost\"),  \n",
    "            col(\"emp.TrainingHours\"),\n",
    "            col(\"emp.JobLevel\"),\n",
    "            col(\"emp.TrainingOutcome\"),\n",
    "            col(\"emp.JobSatisfaction\"),\n",
    "            col(\"emp.modified_date\"),\n",
    "            col(\"emp.is_deleted\")\n",
    "        ).withColumn(\"FactHRTrainingKey\", row_number().over(Window.orderBy(lit(1))))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Incremental Load for Fact_HR_Training\n",
    "fact_hr_training_table_path = f\"{gold_path}/Fact_HR_Training\"\n",
    "fact_hr_training = create_fact_hr_training()\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, fact_hr_training_table_path):\n",
    "    delta_table = DeltaTable.forPath(spark, fact_hr_training_table_path)\n",
    "    \n",
    "    delta_table.alias(\"old\").merge(\n",
    "        fact_hr_training.alias(\"new\"),\n",
    "        \"old.Employee_Key = new.Employee_Key AND old.Training_Key = new.Training_Key AND old.Job_Key = new.Job_Key\"  \n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"Training_Cost\": col(\"new.Training_Cost\"),\n",
    "        \"TrainingHours\": col(\"new.TrainingHours\"),  \n",
    "        \"JobLevel\": col(\"new.JobLevel\"),\n",
    "        \"TrainingOutcome\": col(\"new.TrainingOutcome\"),\n",
    "        \"JobSatisfaction\": col(\"new.JobSatisfaction\"),\n",
    "        \"modified_date\": col(\"new.modified_date\"),\n",
    "        \"is_deleted\": col(\"new.is_deleted\")\n",
    "    }).whenNotMatchedInsert(values={\n",
    "        \"Employee_Key\": col(\"new.Employee_Key\"),\n",
    "        \"Training_Key\": col(\"new.Training_Key\"),\n",
    "        \"Department_Key\": col(\"new.Department_Key\"),\n",
    "        \"Location_Key\": col(\"new.Location_Key\"),\n",
    "        \"Job_Key\": col(\"new.Job_Key\"),  \n",
    "        \"Time_Key\": col(\"new.Time_Key\"),\n",
    "        \"Training_Cost\": col(\"new.Training_Cost\"),\n",
    "        \"TrainingHours\": col(\"new.TrainingHours\"), \n",
    "        \"JobLevel\": col(\"new.JobLevel\"),\n",
    "        \"TrainingOutcome\": col(\"new.TrainingOutcome\"),\n",
    "        \"JobSatisfaction\": col(\"new.JobSatisfaction\"),\n",
    "        \"modified_date\": col(\"new.modified_date\"),\n",
    "        \"is_deleted\": col(\"new.is_deleted\")\n",
    "    }).execute()\n",
    "else:\n",
    "    fact_hr_training.write.format(\"delta\").mode(\"overwrite\").save(fact_hr_training_table_path)\n",
    "\n",
    "print(\"Fact_HR_Training processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adc77954-5668-4dba-8758-27b9bc6be216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 864433068163093,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver To Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
