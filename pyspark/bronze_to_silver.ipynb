{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0fec69-3cff-4e02-84a7-238e3a10dc00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/testsynapsehieum has been unmounted.\n",
      "Mounted /mnt/testsynapsehieum successfully\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": os.getenv(\"AZURE_CLIENT_ID\"),  \n",
    "    \"fs.azure.account.oauth2.client.secret\": os.getenv(\"AZURE_CLIENT_SECRET\"), \n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{os.getenv('AZURE_TERNANT_ID')}/oauth2/v2.0/token\" \n",
    "}\n",
    "container = os.getenv(\"AZURE_CONTAINER_NAME\")\n",
    "storage_account = os.getenv(\"AZURE_STORAGE_ACCOUNT\")\n",
    "mount_point = f\"/mnt/{os.getenv('AZURE_CONTAINER_NAME')}\"   \n",
    "\n",
    "existing_mounts = [mnt.mountPoint for mnt in dbutils.fs.mounts()]\n",
    "if mount_point in existing_mounts:\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=f\"abfss://{container}@{storage_account}.dfs.core.windows.net/\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n",
    "print(f\"Mounted {mount_point} successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0068757-6287-4512-8c6b-4ece0c42dd56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Incremental Load from Bronze to Silver (Parquet) with Data Transformations\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, max, upper, lit, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Bronze_to_Silver\").getOrCreate()\n",
    "\n",
    "# Define storage paths\n",
    "bronze_path = \"/mnt/testsynapsehieum/bronze\"\n",
    "silver_path = \"/mnt/testsynapsehieum/silver\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7845f627-f222-4f31-8b98-249be8d848df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables found in Bronze: ['HR.BusinessTravel.parquet', 'HR.Department.parquet', 'HR.Employee.parquet', 'HR.Job.parquet', 'HR.Location.parquet', 'HR.Shift.parquet', 'HR.Training.parquet']\n"
     ]
    }
   ],
   "source": [
    "# List all available Parquet files in the Bronze layer\n",
    "bronze_tables = [f.name for f in dbutils.fs.ls(bronze_path) if f.name.endswith(\".parquet\")]\n",
    "\n",
    "# Display detected tables\n",
    "print(\"Tables found in Bronze:\", bronze_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cccf0f95-547d-4f85-8170-4c607beb217a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: HR.BusinessTravel\n",
      "No new data found for HR.BusinessTravel. Skipping update.\n",
      "Processing table: HR.Department\n",
      "No new data found for HR.Department. Skipping update.\n",
      "Processing table: HR.Employee\n",
      "No new data found for HR.Employee. Skipping update.\n",
      "Processing table: HR.Job\n",
      "No new data found for HR.Job. Skipping update.\n",
      "Processing table: HR.Location\n",
      "No new data found for HR.Location. Skipping update.\n",
      "Processing table: HR.Shift\n",
      "No new data found for HR.Shift. Skipping update.\n",
      "Processing table: HR.Training\n",
      "No new data found for HR.Training. Skipping update.\n",
      "Incremental Load from Bronze to Silver completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Process Each Table\n",
    "for table in bronze_tables:\n",
    "    table_name = table.replace(\".parquet\", \"\")  # Remove file extension\n",
    "    bronze_table_path = f\"{bronze_path}/{table}\"  # Define Bronze table path\n",
    "    silver_table_folder = f\"{silver_path}/{table_name}\"  # Create folder for each table in Silver\n",
    "\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "\n",
    "    # Load Bronze data\n",
    "    df_bronze = spark.read.format(\"parquet\").load(bronze_table_path)\n",
    "\n",
    "    # Check if Silver table exists\n",
    "    try:\n",
    "        df_silver = spark.read.format(\"parquet\").load(silver_table_folder)\n",
    "        silver_exists = True\n",
    "    except:\n",
    "        silver_exists = False\n",
    "\n",
    "    # Determine incremental load\n",
    "    if silver_exists:\n",
    "        latest_timestamp = df_silver.select(max(col(\"modified_date\"))).collect()[0][0]\n",
    "\n",
    "        # Lọc dữ liệu mới từ Bronze (chỉ giữ lại các bản ghi có modified_date mới hơn)\n",
    "        df_new_data = df_bronze.filter(col(\"modified_date\") > latest_timestamp)\n",
    "\n",
    "        # Nếu không có dữ liệu mới, bỏ qua\n",
    "        if df_new_data.count() == 0:\n",
    "            print(f\"No new data found for {table_name}. Skipping update.\")\n",
    "            continue\n",
    "    else:\n",
    "        df_new_data = df_bronze  # Full load for first execution\n",
    "\n",
    "    # APPLY DATA TRANSFORMATIONS (Only if the column exists)\n",
    "    if \"name\" in df_new_data.columns:\n",
    "        df_new_data = df_new_data.withColumn(\"name\", upper(col(\"name\")))\n",
    "\n",
    "    if \"is_deleted\" in df_new_data.columns:\n",
    "        df_new_data = df_new_data.withColumn(\n",
    "            \"is_deleted\",\n",
    "            when(col(\"is_deleted\").isNull(), lit(0)).otherwise(col(\"is_deleted\").cast(\"int\"))\n",
    "        )  # Ensure is_deleted is 0 or 1 (integer type)\n",
    "\n",
    "    # Append new data to Silver layer (Parquet format)\n",
    "    df_new_data.write.format(\"parquet\").mode(\"append\").save(silver_table_folder)\n",
    "    print(f\"Table {table_name} processed successfully with incremental data.\")\n",
    "\n",
    "print(\"Incremental Load from Bronze to Silver completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze To Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
